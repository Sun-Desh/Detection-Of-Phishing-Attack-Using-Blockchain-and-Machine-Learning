{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ff1131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and preprocessing dataset...\n",
      "Loaded dataset_phishing.csv with 11430 rows\n",
      "\n",
      "Preprocessing steps:\n",
      "1. Converting labels...\n",
      "2. Cleaning URLs...\n",
      "3. Validating URLs...\n",
      "Dataset before URL validation: 11430\n",
      "Dataset after URL validation: 11430\n",
      "\n",
      "4. Splitting dataset...\n",
      "\n",
      "Dataset Statistics:\n",
      "--------------------------------------------------\n",
      "Total valid URLs: 11430\n",
      "Training set size: 5715\n",
      "Testing set size: 5715\n",
      "\n",
      "Training Set Label Distribution:\n",
      "label\n",
      "1    0.500087\n",
      "0    0.499913\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Testing Set Label Distribution:\n",
      "label\n",
      "0    0.500087\n",
      "1    0.499913\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "URL Length Statistics:\n",
      "Training set:\n",
      "count    5715.000000\n",
      "mean       61.677690\n",
      "std        60.467361\n",
      "min        12.000000\n",
      "25%        32.500000\n",
      "50%        47.000000\n",
      "75%        70.000000\n",
      "max      1641.000000\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "\n",
    "print(\"\\nLoading and preprocessing dataset...\")\n",
    "\n",
    "\n",
    "df = pd.read_csv('dataset_phishing.csv')\n",
    "print(f\"Loaded dataset_phishing.csv with {len(df)} rows\")\n",
    "\n",
    "\n",
    "print(\"\\nPreprocessing steps:\")\n",
    "\n",
    "\n",
    "print(\"1. Converting labels...\")\n",
    "df['label'] = (df['status'] == 'phishing').astype(int)\n",
    "\n",
    "\n",
    "print(\"2. Cleaning URLs...\")\n",
    "df['text'] = df['url'].fillna('').astype(str)\n",
    "\n",
    "\n",
    "print(\"3. Validating URLs...\")\n",
    "def is_valid_url(url):\n",
    "    \"\"\"Basic URL validation\"\"\"\n",
    "    if not isinstance(url, str):\n",
    "        return False\n",
    "    return len(url) > 5\n",
    "\n",
    "print(f\"Dataset before URL validation: {len(df)}\")\n",
    "df = df[df['text'].apply(is_valid_url)].copy()\n",
    "print(f\"Dataset after URL validation: {len(df)}\")\n",
    "\n",
    "\n",
    "print(\"\\n4. Splitting dataset...\")\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.5,  \n",
    "    random_state=42,\n",
    "    stratify=df['label']  \n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total valid URLs: {len(df)}\")\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Testing set size: {len(test_df)}\")\n",
    "\n",
    "print(\"\\nTraining Set Label Distribution:\")\n",
    "print(train_df['label'].value_counts(normalize=True))\n",
    "print(\"\\nTesting Set Label Distribution:\")\n",
    "print(test_df['label'].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "print(\"\\nURL Length Statistics:\")\n",
    "url_lengths = train_df['text'].str.len()\n",
    "print(\"Training set:\")\n",
    "print(url_lengths.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5363c5ae",
   "metadata": {},
   "source": [
    "Dataset link :- https://www.kaggle.com/datasets/shashwatwork/web-page-phishing-detection-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec82b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n",
      "\n",
      "Optimized max_length set to 256 based on URL length distribution\n",
      "\n",
      "Initializing model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting datasets to PyTorch format...\n",
      "Created initial dataset with 5715 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e3422a4a62439d8a0a9b3bb0516b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed tokenization\n",
      "Dataset format set for training\n",
      "Created initial dataset with 5715 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc01c5b6f5584c8895c88912385fff80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed tokenization\n",
      "Dataset format set for training\n",
      "Created training dataset with 5715 samples\n",
      "Created testing dataset with 5715 samples\n",
      "\n",
      "Class weights for balanced training: tensor([1.0002, 0.9998])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Preparing datasets...\")\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "url_lengths = train_df['text'].str.len()\n",
    "max_length = min(256, int(url_lengths.quantile(0.99)))  \n",
    "print(f\"\\nOptimized max_length set to {max_length} based on URL length distribution\")\n",
    "\n",
    "def df_to_dataset(df, batch_size=2000):  \n",
    "    \"\"\"Convert DataFrame to Dataset with tokenization\"\"\"\n",
    "    try:\n",
    "        \n",
    "        ds = Dataset.from_pandas(df[['text', 'label']].reset_index(drop=True))\n",
    "        print(f\"Created initial dataset with {len(ds)} samples\")\n",
    "        \n",
    "        \n",
    "        def tokenize(batch):\n",
    "            return tokenizer(\n",
    "                batch['text'], \n",
    "                truncation=True, \n",
    "                padding='max_length', \n",
    "                max_length=max_length,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=None  \n",
    "            )\n",
    "        \n",
    "        \n",
    "        ds = ds.map(tokenize, batched=True, batch_size=batch_size, num_proc=4)\n",
    "        print(\"Completed tokenization\")\n",
    "        \n",
    "        \n",
    "        ds = ds.rename_column('label', 'labels')\n",
    "        ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "        print(\"Dataset format set for training\")\n",
    "        \n",
    "        return ds\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in dataset preparation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "print(\"\\nInitializing model and tokenizer...\")\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.2,  \n",
    "    attention_probs_dropout_prob=0.2\n",
    ")\n",
    "\n",
    "print(\"\\nConverting datasets to PyTorch format...\")\n",
    "train_dataset = df_to_dataset(train_df)\n",
    "test_dataset = df_to_dataset(test_df)\n",
    "print(f\"Created training dataset with {len(train_dataset)} samples\")\n",
    "print(f\"Created testing dataset with {len(test_dataset)} samples\")\n",
    "\n",
    "labels = train_df['label'].values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"\\nClass weights for balanced training:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa41a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from: ./phishing_detection_model_new\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = './phishing_detection_model_new'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "max_length = 128\n",
    "\n",
    "print(\"Model loaded successfully from:\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4ed0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training configuration...\n",
      "\n",
      "Advanced Training Configuration:\n",
      "Total training samples: 5715\n",
      "Total testing samples: 5715\n",
      "Batch size: 32\n",
      "Gradient accumulation steps: 2\n",
      "Effective batch size: 64\n",
      "Number of epochs: 4\n",
      "Initial learning rate: 5e-05\n",
      "Learning rate scheduler: SchedulerType.COSINE_WITH_RESTARTS\n",
      "Total training steps: 357\n",
      "\n",
      "Optimizations enabled:\n",
      "- Adaptive learning rates with cosine restarts\n",
      "- Label smoothing\n",
      "- Increased worker threads\n",
      "- Regular evaluation and model selection\n",
      "- More frequent checkpoints (every 100 steps)\n",
      "- Memory optimizations\n",
      "\n",
      "Advanced Training Configuration:\n",
      "Total training samples: 5715\n",
      "Total testing samples: 5715\n",
      "Batch size: 32\n",
      "Gradient accumulation steps: 2\n",
      "Effective batch size: 64\n",
      "Number of epochs: 4\n",
      "Initial learning rate: 5e-05\n",
      "Learning rate scheduler: SchedulerType.COSINE_WITH_RESTARTS\n",
      "Total training steps: 357\n",
      "\n",
      "Optimizations enabled:\n",
      "- Adaptive learning rates with cosine restarts\n",
      "- Label smoothing\n",
      "- Increased worker threads\n",
      "- Regular evaluation and model selection\n",
      "- More frequent checkpoints (every 100 steps)\n",
      "- Memory optimizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandy/Desktop/thesis/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up training configuration...\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    try:\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        \n",
    "        \n",
    "        tn = ((preds == 0) & (labels == 0)).sum()\n",
    "        tp = ((preds == 1) & (labels == 1)).sum()\n",
    "        fn = ((preds == 0) & (labels == 1)).sum()\n",
    "        fp = ((preds == 1) & (labels == 0)).sum()\n",
    "        \n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'specificity': specificity\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing metrics: {str(e)}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "\n",
    "class AdaptiveTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, max_grad_norm=1.0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        try:\n",
    "            \n",
    "            for k, v in inputs.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    inputs[k] = v.to('cpu')\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "           \n",
    "            if return_outputs and not isinstance(outputs, dict):\n",
    "                outputs = {'logits': outputs[0], 'hidden_states': outputs[1] if len(outputs) > 1 else None}\n",
    "            \n",
    "            if 'labels' in inputs:\n",
    "                labels = inputs['labels']\n",
    "                logits = outputs['logits'] if isinstance(outputs, dict) else outputs[0]\n",
    "                \n",
    "                \n",
    "                if self.class_weights is not None:\n",
    "                    loss_fct = torch.nn.CrossEntropyLoss(\n",
    "                        weight=self.class_weights,\n",
    "                        label_smoothing=0.1  \n",
    "                    )\n",
    "                    loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "                else:\n",
    "                    loss = outputs['loss'] if isinstance(outputs, dict) else outputs[1]\n",
    "            else:\n",
    "                loss = outputs['loss'] if isinstance(outputs, dict) else outputs[1]\n",
    "            \n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in loss computation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "os.makedirs('./logs', exist_ok=True)\n",
    "\n",
    "\n",
    "batch_size = 32  \n",
    "grad_acc_steps = 2  \n",
    "num_epochs = 4  \n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size * 2,  \n",
    "    gradient_accumulation_steps=grad_acc_steps,\n",
    "    learning_rate=5e-5,  \n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,  \n",
    "    save_total_limit=2,\n",
    "    eval_steps=100,  \n",
    "    save_steps=100, \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_num_workers=4,  \n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine_with_restarts',  \n",
    "    report_to='none',  \n",
    "    do_eval=True,  \n",
    "    save_strategy='steps',  \n",
    "    eval_strategy='steps'  \n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nAdvanced Training Configuration:\")\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "print(f\"Total testing samples: {len(test_df)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Gradient accumulation steps: {grad_acc_steps}\")\n",
    "print(f\"Effective batch size: {batch_size * grad_acc_steps}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Initial learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Learning rate scheduler: {training_args.lr_scheduler_type}\")\n",
    "\n",
    "\n",
    "total_steps = len(train_df) * num_epochs / (batch_size * grad_acc_steps)\n",
    "print(f\"Total training steps: {int(total_steps)}\")\n",
    "\n",
    "print(\"\\nOptimizations enabled:\")\n",
    "print(\"- Adaptive learning rates with cosine restarts\")\n",
    "print(\"- Label smoothing\")\n",
    "print(\"- Increased worker threads\")\n",
    "print(\"- Regular evaluation and model selection\")\n",
    "print(f\"- More frequent checkpoints (every {training_args.save_steps} steps)\")\n",
    "print(\"- Memory optimizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e00025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process...\n",
      "\n",
      "Dataset Information:\n",
      "Training samples: 5715\n",
      "└── Phishing URLs: 2858\n",
      "└── Legitimate URLs: 2857\n",
      "\n",
      "Testing samples: 5715\n",
      "└── Phishing URLs: 2857\n",
      "└── Legitimate URLs: 2858\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandy/Desktop/thesis/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [360/360 5:10:59, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.274800</td>\n",
       "      <td>0.359939</td>\n",
       "      <td>0.918985</td>\n",
       "      <td>0.923306</td>\n",
       "      <td>0.876415</td>\n",
       "      <td>0.975499</td>\n",
       "      <td>0.862491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.322151</td>\n",
       "      <td>0.934383</td>\n",
       "      <td>0.931831</td>\n",
       "      <td>0.969365</td>\n",
       "      <td>0.897095</td>\n",
       "      <td>0.971659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.307311</td>\n",
       "      <td>0.947507</td>\n",
       "      <td>0.947183</td>\n",
       "      <td>0.952887</td>\n",
       "      <td>0.941547</td>\n",
       "      <td>0.953464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandy/Desktop/thesis/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/sandy/Desktop/thesis/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/sandy/Desktop/thesis/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Statistics:\n",
      "Total steps completed: 360\n",
      "Total training time: 18719.51 seconds\n",
      "Training samples/second: 1.22\n",
      "Final training loss: 0.2685\n",
      "\n",
      "Evaluating model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandy/Desktop/thesis/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 14:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Metrics:\n",
      "Accuracy: 0.9475\n",
      "F1 Score: 0.9472\n",
      "Precision: 0.9529\n",
      "Recall: 0.9415\n",
      "Specificity: 0.9535\n",
      "\n",
      "Best model saved to ./phishing_detection_model_new\n",
      "Matplotlib not available for plotting learning curves\n",
      "\n",
      "Best model saved to ./phishing_detection_model_new\n",
      "Matplotlib not available for plotting learning curves\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training process...\")\n",
    "\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"└── Phishing URLs: {len(train_df[train_df['label'] == 1])}\")\n",
    "print(f\"└── Legitimate URLs: {len(train_df[train_df['label'] == 0])}\")\n",
    "print(f\"\\nTesting samples: {len(test_df)}\")\n",
    "print(f\"└── Phishing URLs: {len(test_df[test_df['label'] == 1])}\")\n",
    "print(f\"└── Legitimate URLs: {len(test_df[test_df['label'] == 0])}\")\n",
    "\n",
    "\n",
    "trainer = AdaptiveTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    "    max_grad_norm=1.0  \n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "\n",
    "print(\"\\nTraining Statistics:\")\n",
    "print(f\"Total steps completed: {train_result.global_step}\")\n",
    "print(f\"Total training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nEvaluating model on test set...\")\n",
    "eval_result = trainer.evaluate()\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"Accuracy: {eval_result['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {eval_result['eval_f1']:.4f}\")\n",
    "print(f\"Precision: {eval_result['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {eval_result['eval_recall']:.4f}\")\n",
    "print(f\"Specificity: {eval_result['eval_specificity']:.4f}\")\n",
    "\n",
    "\n",
    "output_dir = './phishing_detection_model_new'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"\\nBest model saved to {output_dir}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    \n",
    "    history = pd.DataFrame(trainer.state.log_history)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['step'], history['loss'], label='Training Loss')\n",
    "    plt.title('Training Loss over Time')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    eval_history = history[history['eval_accuracy'].notna()]\n",
    "    metrics = ['eval_accuracy', 'eval_f1', 'eval_precision', 'eval_recall']\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for metric in metrics:\n",
    "        plt.plot(eval_history['step'], eval_history[metric], label=metric.replace('eval_', ''))\n",
    "    plt.title('Evaluation Metrics over Time')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Matplotlib not available for plotting learning curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfaddd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced URL Testing:\n",
      "================================================================================\n",
      "\n",
      "Original URL: http://www.google.com\n",
      "Processed URL: http://www.google.com\n",
      "Prediction: legitimate\n",
      "Confidence: 95.83% (HIGH)\n",
      "Suspicious Patterns: 0\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 2\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: http://paypal-secure-login.com/verify\n",
      "Processed URL: http://paypal-secure-login.com/verify\n",
      "Prediction: phishing\n",
      "Confidence: 100.00% (HIGH)\n",
      "Suspicious Patterns: 4\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: www.facebook.com\n",
      "Processed URL: http://www.facebook.com\n",
      "Prediction: legitimate\n",
      "Confidence: 95.85% (HIGH)\n",
      "Suspicious Patterns: 0\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 2\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: secure-banking.com.suspicious.net/login\n",
      "Processed URL: http://www.secure-banking.com.suspicious.net/login\n",
      "Prediction: phishing\n",
      "Confidence: 100.00% (HIGH)\n",
      "Suspicious Patterns: 3\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 4\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: https://account-verify.paypal.com.suspicious.net\n",
      "Processed URL: https://account-verify.paypal.com.suspicious.net\n",
      "Prediction: phishing\n",
      "Confidence: 100.00% (HIGH)\n",
      "Suspicious Patterns: 3\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 4\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: http://mybank.com@phishing.com\n",
      "Processed URL: http://mybank.com@phishing.com\n",
      "Prediction: phishing\n",
      "Confidence: 83.71% (MEDIUM)\n",
      "Suspicious Patterns: 0\n",
      "Suspicious Characters: Yes\n",
      "Number of Dots: 2\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: https://ku.ac.bd\n",
      "Processed URL: https://ku.ac.bd\n",
      "Prediction: legitimate\n",
      "Confidence: 95.24% (HIGH)\n",
      "Suspicious Patterns: 0\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 2\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: http://verify-account.secure-login.com\n",
      "Processed URL: http://verify-account.secure-login.com\n",
      "Prediction: phishing\n",
      "Confidence: 100.00% (HIGH)\n",
      "Suspicious Patterns: 4\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 2\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: secure-banking.com.suspicious.net/login\n",
      "Processed URL: http://www.secure-banking.com.suspicious.net/login\n",
      "Prediction: phishing\n",
      "Confidence: 100.00% (HIGH)\n",
      "Suspicious Patterns: 3\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 4\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: https://account-verify.paypal.com.suspicious.net\n",
      "Processed URL: https://account-verify.paypal.com.suspicious.net\n",
      "Prediction: phishing\n",
      "Confidence: 100.00% (HIGH)\n",
      "Suspicious Patterns: 3\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 4\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: http://mybank.com@phishing.com\n",
      "Processed URL: http://mybank.com@phishing.com\n",
      "Prediction: phishing\n",
      "Confidence: 83.71% (MEDIUM)\n",
      "Suspicious Patterns: 0\n",
      "Suspicious Characters: Yes\n",
      "Number of Dots: 2\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: https://ku.ac.bd\n",
      "Processed URL: https://ku.ac.bd\n",
      "Prediction: legitimate\n",
      "Confidence: 95.24% (HIGH)\n",
      "Suspicious Patterns: 0\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 2\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: http://verify-account.secure-login.com\n",
      "Processed URL: http://verify-account.secure-login.com\n",
      "Prediction: phishing\n",
      "Confidence: 100.00% (HIGH)\n",
      "Suspicious Patterns: 4\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 2\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: www.prothomalo.com\n",
      "Processed URL: http://www.prothomalo.com\n",
      "Prediction: legitimate\n",
      "Confidence: 95.92% (HIGH)\n",
      "Suspicious Patterns: 0\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 2\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: http://banking.update.security.mydomain.com\n",
      "Processed URL: http://banking.update.security.mydomain.com\n",
      "Prediction: phishing\n",
      "Confidence: 100.00% (HIGH)\n",
      "Suspicious Patterns: 2\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 4\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prediction Statistics:\n",
      "Total URLs tested: 10\n",
      "Phishing predictions: 6\n",
      "Legitimate predictions: 4\n",
      "\n",
      "Confidence Levels:\n",
      "High confidence: 9\n",
      "Medium confidence: 1\n",
      "Low confidence: 0\n",
      "\n",
      "Original URL: www.prothomalo.com\n",
      "Processed URL: http://www.prothomalo.com\n",
      "Prediction: legitimate\n",
      "Confidence: 95.92% (HIGH)\n",
      "Suspicious Patterns: 0\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 2\n",
      "------------------------------------------------------------\n",
      "\n",
      "Original URL: http://banking.update.security.mydomain.com\n",
      "Processed URL: http://banking.update.security.mydomain.com\n",
      "Prediction: phishing\n",
      "Confidence: 100.00% (HIGH)\n",
      "Suspicious Patterns: 2\n",
      "Suspicious Characters: No\n",
      "Number of Dots: 4\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prediction Statistics:\n",
      "Total URLs tested: 10\n",
      "Phishing predictions: 6\n",
      "Legitimate predictions: 4\n",
      "\n",
      "Confidence Levels:\n",
      "High confidence: 9\n",
      "Medium confidence: 1\n",
      "Low confidence: 0\n"
     ]
    }
   ],
   "source": [
    "def preprocess_url(url):\n",
    "    \"\"\"Preprocess URL for consistent format\"\"\"\n",
    "    url = url.lower().strip()\n",
    "    \n",
    "    # Add http:// if no protocol is specified\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        if url.startswith('www.'):\n",
    "            url = 'http://' + url\n",
    "        else:\n",
    "            url = 'http://www.' + url.replace('www.', '')\n",
    "    \n",
    "    return url\n",
    "\n",
    "def extract_url_features(url):\n",
    "    \"\"\"Extract features that might indicate phishing\"\"\"\n",
    "    suspicious_patterns = [\n",
    "        'login', 'signin', 'verify', 'secure', 'account',\n",
    "        'update', 'confirm', 'banking', 'paypal', 'password'\n",
    "    ]\n",
    "    \n",
    "    url_lower = url.lower()\n",
    "    suspicious_count = sum(1 for pattern in suspicious_patterns if pattern in url_lower)\n",
    "    has_suspicious_chars = '@' in url or '%' in url\n",
    "    dots_count = url.count('.')\n",
    "    \n",
    "    return suspicious_count, has_suspicious_chars, dots_count\n",
    "\n",
    "def predict_url(url, max_length=None):\n",
    "    \"\"\"Enhanced URL prediction with better detection\"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = min(256, len(url) + 10)\n",
    "    \n",
    "    # Preprocess URL\n",
    "    processed_url = preprocess_url(url)\n",
    "    \n",
    "    # Get additional features\n",
    "    suspicious_count, has_suspicious_chars, dots_count = extract_url_features(processed_url)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        processed_url,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    predictions = torch.softmax(outputs.logits, dim=1)\n",
    "    predicted_class = torch.argmax(predictions).item()\n",
    "    confidence = predictions[0][predicted_class].item()\n",
    "    \n",
    "    # Adjust confidence based on additional features\n",
    "    if predicted_class == 1:  # If model predicts phishing\n",
    "        if suspicious_count >= 2 or (has_suspicious_chars and dots_count > 2):\n",
    "            confidence = min(confidence + 0.1, 1.0)\n",
    "    else:  # If model predicts legitimate\n",
    "        if suspicious_count >= 2 and dots_count > 3:\n",
    "            confidence = max(confidence - 0.1, 0.0)\n",
    "            predicted_class = 1  # Change to phishing if highly suspicious\n",
    "    \n",
    "    threshold = 0.8\n",
    "    prediction = 'phishing' if predicted_class == 1 else 'legitimate'\n",
    "    if confidence < threshold:\n",
    "        confidence_label = \"LOW\"\n",
    "    elif confidence < 0.9:\n",
    "        confidence_label = \"MEDIUM\"\n",
    "    else:\n",
    "        confidence_label = \"HIGH\"\n",
    "    \n",
    "    return {\n",
    "        'url': url,\n",
    "        'processed_url': processed_url,\n",
    "        'prediction': prediction,\n",
    "        'confidence': f\"{confidence:.2%}\",\n",
    "        'confidence_level': confidence_label,\n",
    "        'sequence_length': max_length,\n",
    "        'suspicious_patterns': suspicious_count,\n",
    "        'suspicious_chars': has_suspicious_chars,\n",
    "        'dots_count': dots_count\n",
    "    }\n",
    "\n",
    "# Test with various URL formats and suspicious patterns\n",
    "test_urls = [\n",
    "    \"http://www.google.com\",  # Legitimate\n",
    "    \"http://paypal-secure-login.com/verify\",  # Phishing\n",
    "    \"www.facebook.com\",  # Legitimate, no protocol\n",
    "    \"secure-banking.com.suspicious.net/login\",  # Phishing\n",
    "    \"https://account-verify.paypal.com.suspicious.net\",  # Phishing\n",
    "    \"http://mybank.com@phishing.com\",  # Phishing with @ symbol\n",
    "    \"https://ku.ac.bd\",  # Legitimate\n",
    "    \"http://verify-account.secure-login.com\",  # Phishing\n",
    "    \"www.prothomalo.com\",  # Legitimate news site\n",
    "    \"http://banking.update.security.mydomain.com\"  # Phishing with multiple subdomains\n",
    "]\n",
    "\n",
    "print(\"Enhanced URL Testing:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "for url in test_urls:\n",
    "    result = predict_url(url)\n",
    "    results.append(result)\n",
    "    print(f\"\\nOriginal URL: {result['url']}\")\n",
    "    print(f\"Processed URL: {result['processed_url']}\")\n",
    "    print(f\"Prediction: {result['prediction']}\")\n",
    "    print(f\"Confidence: {result['confidence']} ({result['confidence_level']})\")\n",
    "    print(f\"Suspicious Patterns: {result['suspicious_patterns']}\")\n",
    "    print(f\"Suspicious Characters: {'Yes' if result['suspicious_chars'] else 'No'}\")\n",
    "    print(f\"Number of Dots: {result['dots_count']}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "predictions_summary = {\n",
    "    'total': len(results),\n",
    "    'phishing': len([r for r in results if r['prediction'] == 'phishing']),\n",
    "    'legitimate': len([r for r in results if r['prediction'] == 'legitimate']),\n",
    "    'high_confidence': len([r for r in results if r['confidence_level'] == 'HIGH']),\n",
    "    'medium_confidence': len([r for r in results if r['confidence_level'] == 'MEDIUM']),\n",
    "    'low_confidence': len([r for r in results if r['confidence_level'] == 'LOW'])\n",
    "}\n",
    "\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "print(f\"Total URLs tested: {predictions_summary['total']}\")\n",
    "print(f\"Phishing predictions: {predictions_summary['phishing']}\")\n",
    "print(f\"Legitimate predictions: {predictions_summary['legitimate']}\")\n",
    "print(f\"\\nConfidence Levels:\")\n",
    "print(f\"High confidence: {predictions_summary['high_confidence']}\")\n",
    "print(f\"Medium confidence: {predictions_summary['medium_confidence']}\")\n",
    "print(f\"Low confidence: {predictions_summary['low_confidence']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
